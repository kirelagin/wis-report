Предварительная обработка
--------------------------

Статья википедии представляет собой текст, отформатированный с помощью вики-разметки. Это позволяет новичкам быстро освоить основы оформления.
К сожалению, у вики-разметки есть серьезный недостаток: она никак не стандартизована. По этой причине абсолютно корректный парсер реализовать невозможно.
Обилие недокументированных возможностей, крайних случаев и ошибок, активно используемых авторами статей, сводит к нулю возможность стандартизации.

Для построения поискового индекса первым делом необходимо отделить слова от разметки. Оказалось, что, в силу изложенного выше, задача эта крайне трудна.
Наш индексатор использует нетривиальный алгоритм удаления вики-разметки и html-тегов, основанный на исходном коде проекта Tanl[@tanl],
и состоящий, по сути, из большого числа реглярных выражений.
После этого все небезопасные с точки зрения HTML символы (`<`, `>`, `&`) экранируются.


Построение индекса и коллекции документов
------------------------------------------

Полученный в результате удаления разметки текст разделяется на токены.
Для облегчения задачи первым делом с помощью токенизтора `PunktSentenceTokenizer` из библиотеки _NLTK_[@nltk] (оригинальное описание которого можно найти в статье [@punkt])
выделяются отдельные предложения. Затем, каждое предложении по-одельности обрабатывается модифицированной версией токенизатора `Penn Treebank`[@treebank].

На следующем этапе полученый поток токенов фильтруется, чтобы исключить из него пунктуацию и стоп-слова, а после — нормализуется.
Нормализация включает в себя приведение слова к основе (стемматизацию) с помощью алгоритма, известного как `Porter2`[@porter2]. Полученные стемы сохраняются
в индекс вместе с указанием идентификаторов документов, в которых они встретились, и точных позиций в этих документах (постингов). В качестве идентификаторов
документов мы используем sha1-хэши ревизий, содержащиеся в файле с экспортированными статьями.

Кроме того, информация о документе (заголовок, текст, размер), а также разбиение на токены записываются в базу данных документов (коллекцию), позволяющую
извлекать все эти сведения по индетификатору документа, то есть по хэшу. Помимо этого, в базе данных хранится мета-информация о коллекции (число документов,
средняя длина документа), необходимая для ранжирования поисковой выдачи (см. далее).


Простой поиск
--------------

Задача системы — по запросу пользователя найти наиболее подходящие (релевантные) документы. Поисковый запрос — это просто строка текста, потому
важно правильно выделить из неё ключевые слова. Запрос токенизируется и нормализуется подобно тому, как это происходит с текстом при индексации;
выделяется полезная для успешного поиска информация. Оставшиеся после фильтрации и нормализации основы используются для извлечения постингов из индекса.

Мы решили, что в условиях конкретной задачи (поиск по электронной энциклопедии) вполне уместно требовать, чтобы все слова из поискового запроса
(точнее, все не отфильрованные в результате предобработки запроса) содержались в выдаваемых документах, то есть итоговое множество документов —
пересечение результатов запросов к индексу для каждого слова в отдельности.

Следует отметить, что в случае более общего поиска по интернету такой подход, возможно, будет приводить к несколько более плохим результатам.


Ранжирование
-------------

Ранжирование документов в нашей поисковой системе основано на алгоритме _BM25_[@okapi] (также известном как _Okapi BM25_).


### BM25

После того, как получено множество документов, удовлетворяющих запросу, для каждого документа ($D$) вычисляется релевантность запросу ($Q$) по формуле

$$\text{score}(D,Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})},$$

где $f(q_i, D)$ — частота термина $q_i$ в документе $D$, $|D|$ — размер документа $D$, $\text{avgdl}$ — средний размер документа в коллекции.
Константы $k_1 = 1.6$ и $b = 0.75$ выбраны в соответствии с рекомендациями, приведенными в [@manning].

$\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5},$ где $N$ — число документов в коллекции, а $n(q_i)$ — число документов, содержащих термин $q_i$.

$f(q_i, D)$ вычисляется как отношение числа вхождений термина в документ к размеру документа; число вхождений термина известно после запроса к индексу,
а размер каждого документа, равно как число документов и их средний размер, хранятся в базе данных документов. $n(q_i)$ может быть определено с помощью
информации, содержащейся в ответе, полученном от индекса.

Такая схема ранжирования работает неплохо, но не достаточно хорошо.


### Дополнительные эвристики

У Википедии есть важная особенность: часто наилучшим ответом на запрос будет статья, название которой в точности совпадает с самим запросом.
Таким образом, при ранжировании необходимо придавать дополнительный вес заголовку статьи.

К примеру, очевидно, что по запросу «engineering» на первом месте следует вывести статью «Engineering». Если использовать только алгоритм _BM25_, то
упомняутая статья оказывается на третьей странице поисковой выдачи (в районе третьего десятка), что, конечно же, нельзя считать удовлетворительным результатом.

#### Эвристика №1 (по основам слов в заголовке)

Первая эвристика поощряет статьи, содержащие в заголовке слова из поискового запроса; чем точнее заголовок соответствует запросу, тем заметнее
статья поднимается наверх в выдаче. Максимальный бонус от этой эвристики получит статья, содержащая в заголовке все термины из запроса
(точнее, все неотфильтрованные в результате преодбработки запроса термины).

Такая специализация ранжирования дает ощутимый положительный результат, но рассмотренный пример статьи «Engineering» показывает, что этого недостаточно,
поскольку статья подниается лишь на вторую страницу. Дело в том, что многие другие статьи («Traffic engineering», «Social engineering», «Network engineering», …)
также получают максимальный бонус.

#### Эвристика №2 (по точным соответствиям в заголовке)

У предыдущей эвристики есть еще один недостаток. Она учитывает только основы слов, а не точные формы. Это позволяет успешно выбирать статьи,
подходящие под тематику запроса, но не сильно влияет на порядок статей внутри одной темы. К примеру, слова «engineering» и «engineer» имеют одинаковую
основу, следовательно оба получат максимальный балл. В то время, как очевидно, что, если пользователь ввел запрос «engineer», то статья «Engineer» более релевантна,
нежели статья «Engineering».

Вторая эвристика работает с неотфильтрованными (только лишь токенизированными) запросом и заголовком. Она учитывает не только формы слов, но и точное число
вхождений каждой формы. Кроме того, поскольку производится лишь минимальная нормализация, а фильтрация не производится вообще, учитываются также
слова из стоп-листа и пунктуация.

Именно благодаря этой эвристике наша поисковая система по запросу «engineer» на первом месте выводит статью «Engineer», а по запросу «engineering» —
статью «Engineering». Эта же эвристика поднимает на первое место статью «Apple I» по запросу «apple I» (хотя «I» — стоп-слово).

Принципиальное отличие от эвристики, описанной в предыдущем разделе, заключается в том, что теперь мы поощряем статьи не за факт наличия
«правильного» слова в заголовке, а также за отсутствие «неправильных» слов. Здесь максимальный бонус получит статья, содержащая в заголовке в точности
все слова (и символы) из запроса в тех же самых формах, _и_ не содержащая ничего другого. Таким образом, по запросу «engineering» статья
«Engineering» окажется в выдаче выше, чем «Network engineering», поскольку вторая содержит лишнее слово. Кроме того, как уже было замечено,
учитывается не только наличие или отсутствие токенов, но и их число. Потому, по запросу «please please» статья о песне «Please Please Me» окажется
выше, чем статья о слове «Please».


Исправление опечаток
---------------------

Простейшее исправление опечаток работает следующим образом. Если при точном поиске основы в индексе не было обнаружено ни одного документа, значит, в запросе, скорее всего,
присуствует опечатка. В этом случае делается нечеткий запрос к индексу, возвращающий постинги всех основ, находящихся на небольшом редакционном расстоянии от искомой.

Оказалось, что такой подход имеет целый ряд недостатков. Во-первых, с ним не будет работать описанная выше под номером 2 эвристика. На первый взгляд, потеря не
велика, но на самом деле качество поиска ухудшается заметно. Во-вторых, такой подход довольно плохо сочетается с выделением основ слов (стеммингом). К примеру,
используемый нами алгоритм стемминга для слова «computer» выдаст основу «comput», а слово «cmputer» не изменит. Как видно, исходные слова различаются
одной буквой, а полученные основы — тремя. Этих двух неприятных фактов достаточно, чтобы сделать вывод о необходимости исправления опечаток _перед_ нормализацией запроса.

#### Исправление опечаток до нормализации запроса


